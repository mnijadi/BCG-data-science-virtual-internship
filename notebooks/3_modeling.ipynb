{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Modeling\n",
    "\n",
    "---\n",
    "\n",
    "## Import packages & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# shows plots in jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# set plot style\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset:  (14606, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consumption_last_year_energy</th>\n",
       "      <th>off_peak_forecast_energy</th>\n",
       "      <th>off_peak_forecast_power</th>\n",
       "      <th>has_gas</th>\n",
       "      <th>consumption_current_energy</th>\n",
       "      <th>active_products</th>\n",
       "      <th>net_margin</th>\n",
       "      <th>antiquity</th>\n",
       "      <th>consumption_current_power</th>\n",
       "      <th>churn</th>\n",
       "      <th>off_peak_mean_energy</th>\n",
       "      <th>off_peak_mean_power</th>\n",
       "      <th>off_peak_diff_energy</th>\n",
       "      <th>off_peak_diff_power</th>\n",
       "      <th>origin</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.114481</td>\n",
       "      <td>40.606701</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>678.99</td>\n",
       "      <td>3</td>\n",
       "      <td>43.648</td>\n",
       "      <td>1</td>\n",
       "      <td>0.124787</td>\n",
       "      <td>40.942265</td>\n",
       "      <td>0.020057</td>\n",
       "      <td>3.700961</td>\n",
       "      <td>lxidpiddsbxsbosboudacockeimpuepw</td>\n",
       "      <td>foosdfpfkusacimwkcsosbicdxkicaua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4660</td>\n",
       "      <td>0.145711</td>\n",
       "      <td>44.311378</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>18.89</td>\n",
       "      <td>6</td>\n",
       "      <td>13.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0.149609</td>\n",
       "      <td>44.311375</td>\n",
       "      <td>-0.003767</td>\n",
       "      <td>0.177779</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>MISSING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>544</td>\n",
       "      <td>0.165794</td>\n",
       "      <td>44.311378</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>6.60</td>\n",
       "      <td>6</td>\n",
       "      <td>13.856</td>\n",
       "      <td>0</td>\n",
       "      <td>0.170512</td>\n",
       "      <td>44.385450</td>\n",
       "      <td>-0.004670</td>\n",
       "      <td>0.177779</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>foosdfpfkusacimwkcsosbicdxkicaua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1584</td>\n",
       "      <td>0.146694</td>\n",
       "      <td>44.311378</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>25.46</td>\n",
       "      <td>6</td>\n",
       "      <td>13.200</td>\n",
       "      <td>0</td>\n",
       "      <td>0.151210</td>\n",
       "      <td>44.400265</td>\n",
       "      <td>-0.004547</td>\n",
       "      <td>0.177779</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>lmkebamcaaclubfxadlmueccxoimlema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4425</td>\n",
       "      <td>0.116900</td>\n",
       "      <td>40.606701</td>\n",
       "      <td>0</td>\n",
       "      <td>52.32</td>\n",
       "      <td>1</td>\n",
       "      <td>47.98</td>\n",
       "      <td>6</td>\n",
       "      <td>19.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0.124174</td>\n",
       "      <td>40.688156</td>\n",
       "      <td>-0.006192</td>\n",
       "      <td>0.162916</td>\n",
       "      <td>kamkkxfxxuwbdslkwifmmcsiusiuosws</td>\n",
       "      <td>MISSING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   consumption_last_year_energy  off_peak_forecast_energy  \\\n",
       "0                             0                  0.114481   \n",
       "1                          4660                  0.145711   \n",
       "2                           544                  0.165794   \n",
       "3                          1584                  0.146694   \n",
       "4                          4425                  0.116900   \n",
       "\n",
       "   off_peak_forecast_power  has_gas  consumption_current_energy  \\\n",
       "0                40.606701        1                        0.00   \n",
       "1                44.311378        0                        0.00   \n",
       "2                44.311378        0                        0.00   \n",
       "3                44.311378        0                        0.00   \n",
       "4                40.606701        0                       52.32   \n",
       "\n",
       "   active_products  net_margin  antiquity  consumption_current_power  churn  \\\n",
       "0                2      678.99          3                     43.648      1   \n",
       "1                1       18.89          6                     13.800      0   \n",
       "2                1        6.60          6                     13.856      0   \n",
       "3                1       25.46          6                     13.200      0   \n",
       "4                1       47.98          6                     19.800      0   \n",
       "\n",
       "   off_peak_mean_energy  off_peak_mean_power  off_peak_diff_energy  \\\n",
       "0              0.124787            40.942265              0.020057   \n",
       "1              0.149609            44.311375             -0.003767   \n",
       "2              0.170512            44.385450             -0.004670   \n",
       "3              0.151210            44.400265             -0.004547   \n",
       "4              0.124174            40.688156             -0.006192   \n",
       "\n",
       "   off_peak_diff_power                            origin  \\\n",
       "0             3.700961  lxidpiddsbxsbosboudacockeimpuepw   \n",
       "1             0.177779  kamkkxfxxuwbdslkwifmmcsiusiuosws   \n",
       "2             0.177779  kamkkxfxxuwbdslkwifmmcsiusiuosws   \n",
       "3             0.177779  kamkkxfxxuwbdslkwifmmcsiusiuosws   \n",
       "4             0.162916  kamkkxfxxuwbdslkwifmmcsiusiuosws   \n",
       "\n",
       "                            channel  \n",
       "0  foosdfpfkusacimwkcsosbicdxkicaua  \n",
       "1                           MISSING  \n",
       "2  foosdfpfkusacimwkcsosbicdxkicaua  \n",
       "3  lmkebamcaaclubfxadlmueccxoimlema  \n",
       "4                           MISSING  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients_df = pd.read_csv('../data/processed/processed_data.csv')\n",
    "print('Shape of the dataset: ', clients_df.shape)\n",
    "clients_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(clients_df.drop('churn', axis=1), clients_df['churn'], test_size=0.2, stratify=clients_df['churn'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    # features to be one hot encoded\n",
    "    ohe_features = ['origin', 'channel']\n",
    "    # features to be log transformed\n",
    "    log_features = ['consumption_last_year_energy', 'off_peak_forecast_energy', 'off_peak_forecast_power', 'consumption_current_energy', 'consumption_current_power']\n",
    "    # features to be standardized\n",
    "    standardize_features = log_features + ['net_margin', 'off_peak_mean_energy', 'off_peak_mean_power', 'off_peak_diff_energy', 'off_peak_diff_power']\n",
    "    \n",
    "    # step 1: one hot encode categorical features\n",
    "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    ohe.fit(X_train[ohe_features])\n",
    "    X_train_ohe = pd.DataFrame(ohe.transform(X_train[ohe_features]), columns=ohe.get_feature_names_out(ohe_features))\n",
    "    X_test_ohe = pd.DataFrame(ohe.transform(X_test[ohe_features]), columns=ohe.get_feature_names_out(ohe_features))\n",
    "    print('Shape of the train set after one hot encoding: ', X_train_ohe.shape)\n",
    "    print('Shape of the test set after one hot encoding: ', X_test_ohe.shape)\n",
    "\n",
    "    # step 2: log transform features\n",
    "    X_train[log_features] = np.log(X_train[log_features]+1)\n",
    "    X_test[log_features] = np.log(X_test[log_features]+1)\n",
    "    \n",
    "    # step 3: standardize features\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[standardize_features])\n",
    "    X_train_scaled = pd.DataFrame(scaler.transform(X_train[standardize_features]), columns=standardize_features)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test[standardize_features]), columns=standardize_features)\n",
    "    print('Shape of the train set after standardization: ', X_train_scaled.shape)\n",
    "    print('Shape of the test set after standardization: ', X_test_scaled.shape)\n",
    "    \n",
    "    # step 4: concatenate all features\n",
    "    X_train_processed = pd.concat([X_train_ohe, X_train_scaled, X_train[['has_gas', 'active_products', 'antiquity']]], axis=1)\n",
    "    X_test_processed = pd.concat([X_test_ohe, X_test_scaled, X_test[['has_gas', 'active_products', 'antiquity']]], axis=1)\n",
    "    print('Shape of the train set after preprocessing: ', X_train_processed.shape)\n",
    "    print('Shape of the test set after preprocessing: ', X_test_processed.shape)\n",
    "\n",
    "    return X_train_processed, X_test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train set after one hot encoding:  (11684, 10)\n",
      "Shape of the test set after one hot encoding:  (2922, 10)\n",
      "Shape of the train set after standardization:  (11684, 10)\n",
      "Shape of the test set after standardization:  (2922, 10)\n",
      "Shape of the train set after preprocessing:  (13992, 23)\n",
      "Shape of the test set after preprocessing:  (5291, 23)\n"
     ]
    }
   ],
   "source": [
    "X_train_processed, X_test_processed = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13992, 23)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11684, 15)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Baseline Model\n",
    "\n",
    "First, let's establish a baseline.\n",
    "Since the churn rate is about 10%, we might think of random guessing, that predicts churn with a probability of 10%. We might also think of a model that always predicts non churn, which would be right 90% of the time.\n",
    "\n",
    "Let's define a metric to evaluate our models.  \n",
    "When we go back to the business problem, we don't want to lose customers, in other words, when there is a company that is likely to churn, we want to be able to identify it.  Recall is the metric that measures that.\n",
    "One other thing is that when we predict that a company will churn, we want to be right, because we'll be giving it a discount of 20%, which is a cost for us. Precision is the metric that measures that.\n",
    "\n",
    "Given that, we'll use the F1 score, which is the harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(y_true, y_pred):\n",
    "    print(f'Accuacy: {accuracy_score(y_true, y_pred):.2f}')\n",
    "    print(f'Precision: {precision_score(y_true, y_pred, zero_division=0):.2f}')\n",
    "    print(f'Recall: {recall_score(y_true, y_pred, zero_division=0):.2f}')\n",
    "    print(f'F1: {f1_score(y_true, y_pred, zero_division=0):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuacy: 0.82\n",
      "Precision: 0.11\n",
      "Recall: 0.11\n",
      "F1: 0.11\n"
     ]
    }
   ],
   "source": [
    "# first baseline\n",
    "baseline_1_guesses = np.random.choice([0, 1], size=len(clients_df), p=[.9, .1])\n",
    "print_scores(clients_df.churn, baseline_1_guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the baseline model has an F1 score of around 10%, with both precision and recall being equal.  \n",
    "We need to achieve better results with out modeling.  \n",
    "Since Logistic Regression is a simple model, let's see if it performs better than this simple baseline, if it does, let it be our actual baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode X_train using scikit learn\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# fit ohe to X_train\n",
    "ohe.fit(X_train)\n",
    "\n",
    "# transform X_train\n",
    "X_train_ohe = ohe.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mnijadi/Projects/energy-company-customer-churn/notebooks/3_modeling.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mnijadi/Projects/energy-company-customer-churn/notebooks/3_modeling.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# fit a logistic regression model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mnijadi/Projects/energy-company-customer-churn/notebooks/3_modeling.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m logreg_pipeline \u001b[39m=\u001b[39m Pipeline(pipeline_steps \u001b[39m+\u001b[39m [(\u001b[39m'\u001b[39m\u001b[39mlogreg\u001b[39m\u001b[39m'\u001b[39m, LogisticRegression(random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m))])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mnijadi/Projects/energy-company-customer-churn/notebooks/3_modeling.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m logreg_pipeline\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mnijadi/Projects/energy-company-customer-churn/notebooks/3_modeling.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m logreg_preds \u001b[39m=\u001b[39m logreg_pipeline\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mnijadi/Projects/energy-company-customer-churn/notebooks/3_modeling.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m print_scores(y_test, logreg_preds)\n",
      "File \u001b[0;32m~/Projects/energy-company-customer-churn/.venv/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/energy-company-customer-churn/.venv/lib/python3.11/site-packages/sklearn/pipeline.py:423\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \n\u001b[1;32m    399\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    422\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> 423\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[1;32m    424\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/energy-company-customer-churn/.venv/lib/python3.11/site-packages/sklearn/pipeline.py:377\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    375\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[1;32m    376\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> 377\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    378\u001b[0m     cloned_transformer,\n\u001b[1;32m    379\u001b[0m     X,\n\u001b[1;32m    380\u001b[0m     y,\n\u001b[1;32m    381\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    382\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    383\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[1;32m    384\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[1;32m    385\u001b[0m )\n\u001b[1;32m    386\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/Projects/energy-company-customer-churn/.venv/lib/python3.11/site-packages/joblib/memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/energy-company-customer-churn/.venv/lib/python3.11/site-packages/sklearn/pipeline.py:957\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 957\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    958\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    959\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Projects/energy-company-customer-churn/.venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/Projects/energy-company-customer-churn/.venv/lib/python3.11/site-packages/sklearn/base.py:919\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    917\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 919\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Projects/energy-company-customer-churn/.venv/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:839\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 839\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[0;32m~/Projects/energy-company-customer-churn/.venv/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/energy-company-customer-churn/.venv/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:903\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[1;32m    902\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n\u001b[0;32m--> 903\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    904\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot center sparse matrices: pass `with_mean=False` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    905\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minstead. See docstring for motivation and alternatives.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    906\u001b[0m         )\n\u001b[1;32m    907\u001b[0m     sparse_constructor \u001b[39m=\u001b[39m (\n\u001b[1;32m    908\u001b[0m         sparse\u001b[39m.\u001b[39mcsr_matrix \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mformat \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m sparse\u001b[39m.\u001b[39mcsc_matrix\n\u001b[1;32m    909\u001b[0m     )\n\u001b[1;32m    911\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_std:\n\u001b[1;32m    912\u001b[0m         \u001b[39m# First pass\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives."
     ]
    }
   ],
   "source": [
    "# fit a logistic regression model\n",
    "logreg_pipeline = Pipeline(pipeline_steps + [('logreg', LogisticRegression(random_state=42))])\n",
    "logreg_pipeline.fit(X_train, y_train)\n",
    "logreg_preds = logreg_pipeline.predict(X_test)\n",
    "print_scores(y_test, logreg_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
